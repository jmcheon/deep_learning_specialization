## week2 -  Optimization Algorithms
### Summary
> Develop your deep learning toolbox by adding more advanced optimizations, random minibatching and learning rate decay scheduling to speed up your model.
> - Learning Objectives
>    - Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSprop and Adam
>    - Use random minibatches to accelerate convergence and improve optimization
>    - Describe the benefits of learning rate decay and apply it to your optimization

### Table of contents
1. Optimization Algorithms
	- Mini-batch Gradient Descent
	- Exponentially weighted average
	- Gradient descent with momentum
	- RMSprop
	- Adam
	- Learning rate decay
